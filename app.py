import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig
from accelerate import infer_auto_device_map
import os

# --- Configura√ß√£o da P√°gina do Streamlit ---
st.set_page_config(
    page_title="Gemma 3n Chat",
    page_icon="ü§ñ",
    layout="wide"
)

st.title("ü§ñ Chat com Gemma 3n")
st.caption("Um aplicativo para interagir com o modelo google/gemma-3n-E2B-it.")

# --- Fun√ß√£o de Carregamento do Modelo com Cache ---
# @st.cache_resource garante que o modelo seja carregado apenas uma vez.
@st.cache_resource
def load_model():
    """
    Carrega o modelo e o tokenizador com offloading manual.
    Esta fun√ß√£o ser√° executada apenas uma vez gra√ßas ao cache do Streamlit.
    """
    # Tenta obter o token do Hugging Face dos segredos do Streamlit
    huggingface_token = st.secrets.get("HUGGINGFACE_TOKEN")
    if not huggingface_token:
        st.error("Token do Hugging Face n√£o encontrado! Por favor, adicione-o aos segredos do seu Space.")
        st.stop()
        
    model_id = "google/gemma-3n-E2B-it"

    # Configura√ß√£o de Quantiza√ß√£o
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    # Carrega a configura√ß√£o do modelo
    config = AutoConfig.from_pretrained(model_id, token=huggingface_token)

    # Estrat√©gia de Offloading Manual
    with torch.device("meta"):
        meta_model = AutoModelForCausalLM.from_config(config)

    device_map = infer_auto_device_map(
        meta_model,
        max_memory={0: "14GiB", "cpu": "10GiB"},
        dtype='bfloat16',
        token=huggingface_token
    )
    
    # Carrega o modelo de verdade usando o mapa customizado
    tokenizer = AutoTokenizer.from_pretrained(model_id, token=huggingface_token)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        token=huggingface_token,
        quantization_config=quantization_config,
        device_map=device_map,
    )
    
    return model, tokenizer

# --- L√≥gica Principal do Aplicativo ---

# Exibe um spinner enquanto o modelo √© carregado pela primeira vez
with st.spinner("Carregando o modelo... Isso pode levar alguns minutos na primeira vez."):
    model, tokenizer = load_model()

st.success("Modelo carregado com sucesso!")
st.markdown("---")

# Inicializa o hist√≥rico do chat na sess√£o do Streamlit
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibe as mensagens do hist√≥rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a entrada do usu√°rio
if prompt := st.chat_input("Qual a sua pergunta?"):
    # Adiciona a mensagem do usu√°rio ao hist√≥rico e exibe
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Gera a resposta do modelo
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            chat = [{"role": "user", "content": prompt}]
            prompt_formatado = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer.encode(prompt_formatado, add_special_tokens=False, return_tensors="pt").to("cuda")

            outputs = model.generate(input_ids=inputs, max_new_tokens=1024)
            resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)
            resposta_limpa = resposta.split("model\n")[-1].strip()
            
            st.markdown(resposta_limpa)
    
    # Adiciona a resposta do modelo ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": resposta_limpa})
